1. 编译：
	1.1 opencv3.4.5: set(CMAKE_PREFIX_PATH) → 修改camera_model的cmakelists.txt的include opencv
		set(CMAKE_PREFIX_PATH "/usr/local/include/opencv3.4.5")
		find_package(OpenCV 3.4)
		if(NOT OpenCV_FOUND)
		    find_package(OpenCV 3.0)
		    if(NOT OpenCV_FOUND)
		        message(FATAL_ERROR "OpenCV > 3.0 not found.")
		    endif()
		endif()
		MESSAGE("OPENCV VERSION:")
		MESSAGE(${OpenCV_VERSION})
	1.2 ceres: https://github.com/ceres-solver/ceres-solver
		1.2.1 mkdir build
		1.2.2 cd build
		1.2.3 cmake ..
		1.2.4 make -j8
		1.2.5 make test
		1.2.6 sudo make install
	1.3 编译vins-mono
		1.3.1 mkdir -p catkin_ws/src
		1.3.2 将vins-mono文件夹放到src下
		1.3.3 catkin_init_workspace
		1.3.4 cd ..
		1.3.5 catkin_make
	1.4 运行测试数据
		1.4.1 新开终端：source devel/setup.bash roscore
		1.4.2 新开终端：source devel/setup.bash roslaunch vins_estimator euroc.launch 
		1.4.3 新开终端：source devel/setup.bash roslaunch vins_estimator vins_rviz.launch
		1.4.4 新开终端：source devel/setup.bash rosbag play YOUR_PATH_TO_DATASET/MH_01_easy.bag 
		1.4.5 为了避免每次都source devel/setup.bash：https://blog.csdn.net/github_53583003/article/details/111187288
			1. sudo  gedit ~/.bashrc
			2. 添加：source ~/devel/setup.bash
	1.5 在clion中打开
		1.5.1 在catkin_ws下，source devel/setup.bash
		1.5.2 ./clion.sh
		1.5.3 使用src下面的camkelists.txt加载
		1.5.4 使用当前窗口，不要新建窗口，新建窗口的话，source就是去意义了，然后正常编译即可

2. camera model
	2.1 抽象类：Camera
		2.1.1 负责创建一些公有变量和公有函数，有一些函数是纯虚函数，继承类必须重写
		2.1.2 创建有大量的用于标定的函数，暂时不学习与标定相关的函数
		2.1.3 比较有意思的是函数reprojectionDist：其评价的是两个相机系下的三维点，在同一个相机下投影到图像上，它们的像素坐标的差值作为重投影误差

	2.2 继承类：PINHOLE
		2.2.1 继承抽象的父类Camera，创建针孔相机
		2.2.2 创建有大量的用于标定的函数，暂时不学习与标定相关的函数；其中函数estimateIntrinsics用于实现张正友博士的Z. Zhang, A Flexible New Technique for Camera Calibration, PAMI 2000
		2.2.3 liftProjective快速去畸变：
			1. 巧妙设计了一种快速去畸变的方法，用于快速求解畸变前的归一化相机系坐标
			2. 注意，当畸变为外扩时，如果第一次修正后的归一化坐标不在当前的1/4图像时，此算法无法收敛；但是需要注意的是，一般畸变都不大（至少不会大到出现跨越图像1/4区域的情况），其次，越靠近图像中心，畸变就越小，因此，这种异常情况是不会发生的

	2.3 继承类：Equidistant，使用的是KB8模型
		2.3.1 继承抽象的父类Camera，创建鱼眼相机，注意其参数是k2~k5与一般的k1~k4不同
		2.3.2 创建有大量的用于标定的函数，暂时不学习与标定相关的函数；其中函数estimateIntrinsics用于实现C. Hughes, P. Denny, M. Glavin, and E. Jones, Equidistant Fish-Eye Calibration and Rectification by Vanishing Point；Extraction, PAMI 2010 Find circles from rows of chessboard corners, and for each pair of circles, find vanishing points: v1 and v2. 
		2.3.3 liftProjective
			1. 将像素坐标（u, v）变换为畸变后的归一化相机系坐标(xd, yd)
			2. 使用backprojectSymmetric求解theta和phi
			3. 将(xd, yd)通过theta和phi变化到去畸变前的相机归一化坐标系下，并单位化，从而变换到球面上
		2.3.4 backprojectSymmetric
			1. 相机系下的三维点A与光心的连线与归一化相机系平面交于B1（对应r），经过畸变后与归一化相机系平面交于B2（对应theta_d）；有B1、B2和归一化平面的原点O三点共线
			2. 由于O、B1和B2共线，所以可以直接通过xd, yd计算phi
			3. 对theta的求解：将关于theta的多项式函数令为矩阵A的特征多项式（转化方法见代码），然后调用Eigen的接口求解最小的特征多项式

	2.4 继承类：Scaramuzza --- 暂时不深究
		2.4.1 继承抽象的父类Camera，创建全景相机模型Scaramuzza
		2.4.2 参考教程为：
			1. https://blog.csdn.net/j879159541/article/details/125566948
			2. https://sites.google.com/site/scarabotix/ocamcalib-toolbox

	2.5 继承类：CataCamera --- 暂时不深究
		2.5.1 继承抽象的父类Camera，创建全景相机模型Mei
		2.5.2 参考教程为：
			1. https://blog.csdn.net/j879159541/article/details/125409410
			2. C. Mei, and P. Rives, Single View Point Omnidirectional Camera Calibration from Planar Grids, ICRA 2007

3. feature tracker
	3.1 特征点存储信息：
		3.1.1 像素坐标 → 特征点提取算法
		3.1.2 去畸变后归一化坐标 → 特征点去畸变算法
		3.1.3 特征点id → 光流跟踪算法
		3.1.4 特征点速度 → 时间同步算法使用

	3.2 快门（shutter）: https://blog.csdn.net/weixin_43503508/article/details/108014615
		3.2.1 全局快门（Global Shutter）
			1. 通过整幅场景在同一时间曝光实现的。Sensor所有像素点同时收集光线，同时曝光。即在曝光开始的时候，Sensor开始收集光线；在曝光结束的时候，光线收集电路被切断。然后Sensor值读出即为一幅照片。CCD就是Global shutter工作方式。所有像元同时曝光。
			2. 曝光时间更短，但会增加读出噪声；
		3.2.2 卷帘快门（Rolling Shutter）
			1. 与Global shutter不同，它是通过Sensor逐行曝光的方式实现的。在曝光开始的时候，Sensor逐行扫描逐行进行曝光，直至所有像素点都被曝光。当然，所有的动作在极短的时间内完成。不同行像元的曝光时间不同。
			2. 对于相机厂家，Rolling shutter可以达到更高的帧速，但当曝光不当或物体移动较快时，会出现部分曝光(partial exposure)、斜坡图形(skew)、晃动(wobble) 等现象。这种Rolling shutter方式拍摄出现的现象，就定义为果冻效应。
		3.2.3 曝光时间短的应用（如<500μs）适合Global shutter，曝光时间长（如大于500μs）时，选择rolling shutter可以有更低的噪声和帧速。
		3.2.4 相机曝光时间：
			1. 从快门打开到关闭的时间间隔，在这一段时间内，物体可以在底片上留下影像，曝光时间是看需要而定的，没有长短好坏的说法只有需要的讲法。比如你拍星星的轨迹，就需要很长的曝光时间（可能是几个小时），这样星星的长时间运动轨迹就会在底片上成像。如果你要拍飞驰的汽车清晰的身影就要用很短的时间（通常是几千分之一秒）。曝光时间长的话进的光就多，适合光线条件比较差的情况。曝光时间短则适合光线比较好的情况。
			2. 参考教程：https://baike.baidu.com/item/%E6%9B%9D%E5%85%89%E6%97%B6%E9%97%B4/485425?fr=aladdin
		3.2.5 快门是照相机用来控制感光片有效曝光时间的机构。是照相机的一个重要组成部分，它的结构、形式及功能是衡量照相机档次的一个重要因素。一般而言快门的时间范围越大越好。秒数低适合拍运动中的物体，可轻松抓住急速移动的目标。不过当你要拍的是夜晚的车水马龙，快门时间就要拉长，常见照片中丝绢般的水流效果也要用慢速快门才能拍出来。

	3.3 readParameters
		3.3.1 读取配置文件的信息：image_topic、imu_topic等
		3.3.2 将配置文件路径添加到CAM_NAMES中，实际上这是一个单目VIO系统，CAM_NAMES的size仅仅为1
		3.3.3 设置WINDOW_SIZE(20)、FOCAL_LENGTH(460)

	3.4 readIntrinsicParameter：创建相机，并读取相机的内参数、畸变稀疏和相机类型等

	3.5 img_callback
		3.5.1 第一帧的时候，first_image_flag为true，此时，将时间戳存储到first_image_time和last_image_time，并令first_image_flag为false，然后直接结束
		3.5.2 如果当前帧的时间戳减去上一帧的时间戳（last_image_time）大于1秒（光流法需要两帧之间的间距尽可能小）或者当前时间戳小于上一帧时间戳，那么执行reset为初始值：first_iamge_flag为true，last_image_time为0，pub_count为1，将重启告诉其他模块，并直接结束
		3.5.3 更新last_image_time，并维持发给后端的图像数量不会超过指定的频率；将ros message转化为cv::Mat
		3.5.4 createCLAHE：防止图像太亮或者太暗，不利于提取特征点
		    1. 对图像进行分块后执行equalizeHist
		    	1.1 对图片灰度图做频数统计，计算器频率，然后根据像素灰度值进行排列
		        1.2 根据像素灰度值计算累计概率，利用公式：new_gray = acc_p * (255 - 0)
		        1.3 这样做的目的是使得灰度值之间的间隔更小，即将一些频数较大的灰度值补充给了频数较小的灰度值，从而实现了灰度值的均衡化
		        1.4 由于整体亮度的提升，也会使得局部图像的细节变的模糊，因此最好进行分块的局部均衡化，参考createCLAHE

		    2. 参考教程：
		    	3.1 https://www.cnblogs.com/my-love-is-python/p/10405811.html
		    	3.2 https://en.wikipedia.org/wiki/Histogram_equalization

		3.5.5 readImage:读取图像，并做光流跟踪
			1. 获取当前时间，并且如果EQUALIZE为true，那么就对图像执行createCLAHE，并将结果赋值给img

			2. 更新forw_img，如果是第一帧的话，直接将图像赋值给cur_img和forw_img；如果上一帧有特征点的话：
				2.1 执行光流跟踪calcOpticalFlowPyrLK：使用图像金字塔进行光流追踪，上层结果为下层初值，提升光流追踪的稳定性，这里使用的4层
					2.1.1 光流与特征点的对比：
						1. 光流根据前一张图片的特征点在本张图片中搜索其对应的匹配点，由于不是所有的特征点都能匹配到，因此还需要重新提取一些特征点，因此对于每张图片，其过程为：LK光流追踪，重新提取不足的部分特征点
						2. 对于特征点而言，每张图片都需要提取相同数量的特征点，还需要就是那描述子，然后还需要计算匹配（匹配比较耗时）
						3. 总结：
							3.1 特征点法会比光流耗时
							3.2 光流法在特征点的追踪上会更加鲁棒
							3.3 光流应用于两帧运动很小的时候，而在重定位和回环检测的运动很大，此时更适合用特征点法
							3.4 如果存在遮挡的时候，特征点法会更好；例如，1→2→3，帧1、3看到了某个地图点，2没有看到，如果使用光流法，就不会认为帧1、3看到的是用一个地图点，因为2没有看到，会认为是一个新的地图点；而此时在特征点法中会认为是同一个地图点

				2.2 利用光流跟踪返回的状态值和图像边界删除外点

				2.3 利用reduceVector对cur_pts、forw_pts、ids、cur_un_pts和track_cnt的规模进行删减，并且不会改变原有的顺序：
					2.3.1 利用双指针实现，j指针指向数据，i指针指向状态，只有状态为true的时候，才会将v[j]=v[i]，类似于用v[i]覆盖v[j]
					2.3.2 对数据resize即可

			3. 如果publish这一帧的话，那么：
				3.1 rejectWithF：对极约束剔除外点，第一帧没有对极约束
					3.1.1 只有跟踪到的点的数量不少于8个的时候才会执行
					3.1.2 获取当前帧和前一帧跟踪到的点的liftProjective结果（不同的相机，结果的表示是不同的；比如针孔相机的结果是归一化相机系坐标；鱼眼相机是归一化相机系坐标往单位球上投影的结果），并将结果往虚拟相机上进行投影；构建虚拟相机的目的是使得对极几何等的阈值不受fx和fy的影响，在一个统一的FOCAL_LENGTH下使用一个固定的F_THRESHOLD
					3.1.3 使用opencv的接口findFundamentalMat，在Ransac的模式下构建对极几何，并使用F_THRESHOLD作为阈值；根据对极几何的结果status对prev_pts、cur_pts、forw_pts、cur_un_pts、ids和track_cnt进行处理

				3.2 setMask：特征点均匀化，第一帧不做均匀化
					3.2.1 获取mask，如果没有mask的话，就直接创建一张白图（也就是允许所有的点通过）
					3.2.2 构建变量cnt_pts_id，保存的是pair<跟踪的次数, pair<特征点,id>>，然后根据跟踪的次数进行排序，由大到小；然后清空forw_pts、ids和track_cnt
					3.2.3 对cnt_pts_id进行遍历，根据mask选择是否保留，如果保留这个特征点的话，就记录特征点、id和跟踪的次数；然后在这个特征点上画圈，圈内的特征点不会被获取了

				3.3 根据所需的特征点数量减去跟踪到的特征点数量，获得还需要提取的特征点数量，然后执行特征点提取操作goodFeaturesToTrack：
					3.3.1 

				3.4 将提取的角点添加到相应的容器中addPoints：
					3.4.1 对新提取的特征点，将其添加到forw_pts、ids和track_cnt中，注意ids中添加的是-1，在后面updateID中进行赋值；track_cnt添加的是1，因为是新增的特征点，还没有被跟踪到

			4. 更新cur_img为forw_img，cur_pts为forw_pts；对特征点进行去畸变undistortedPoints:
				4.1 清空cur_un_pts和cur_un_pts_map
				4.2 遍历cur_pts，对每一个特征点进行去畸变，并将结果保存在cur_un_pts和cur_un_pts_map中
				4.3 计算特征点的运动速度：使用去畸变后的图像坐标进行计算
					4.3.1 对第一帧而言，所有特征点的速度为0
					4.3.2 从第二帧开始，首先清空pts_velocity，计算时间间隔dt，遍历cur_un_pts，如果其是跟踪到的，那么计算速度，如果不是跟踪到的，那么速度直接为0，将速度添加到pts_velocity中，更新prev_un_pts_map为cur_un_pts_map

			5. 更新prev_time为cur_time；令人比较迷惑的是，cur_img和cur_pts表示上一帧的图像和特征点，而cur_time表示当前时间

		3.5.6 给新的特征点赋上ID，updateID：第一帧统一在这里进行赋值操作
			1. 如果没有越界，并且是新的特征点，那么id=n_id，并且n_id递增，并返回true；否则，直接返回false

		3.5.7 向后端发送数据：只发送跟踪大于1的特征点，发送的信息有去畸变后的归一化相机系坐标、ID、图形的u坐标、图像的v坐标、u轴的特征点速度和v轴的特征点速度

		3.5.8 可视化：绘制图像和特征点，特征点的颜色根据跟踪次数决定，也就是特征点的颜色并不单一

4. vins_estimator
	4.1 readParameters：
		4.1.1 获取单次优化最大求解时间、单次优化最大迭代次数、关键帧视差（用于确定关键帧的阈值：fsSettings["keyframe_parallax"]/FOCAL_LENGTH）、输出文件的路径（如输出外参动态标定的结果）
		4.1.2 读取IMU参数，如陀螺仪和加速度的噪声和随机游走；图片宽高
		4.1.3 是否估计IMU与相机的外参
			1. 如果ESTIMATE_EXTRINSIC为2，将单位阵添加到RIC，将零向量添加到TIC
			2. 如果ESTIMATE_EXTRINSIC为1或者0，读取给定的旋转和平移分别添加到RIC和TIC中
			3. ESTIMATE_EXTRINSIC=2将会从单位阵开始优化；ESTIMATE_EXTRINSIC=1将会从给定值开始优化；ESTIMATE_EXTRINSIC=0不会优化外参
		4.1.4 读取初始深度INIT_DEPTH、BIAS_ACC_THRESHOLD、BIAS_GYR_THRESHOLD
		4.1.5 读取IMU与相机的时间延时（image + TD = IMU），并读取是否优化时间延时
		4.1.6 读取相机是否是卷帘相机，如果是的话读取卷帘相机的参数TR（读取每行的时间）

	4.2 setParameter：
		4.2.1 将读取的外参赋值给tic和ric
		4.2.2 对ProjectionFactor::sqrt_info和ProjectionTdFactor::sqrt_info均赋值为FOCAL_LENGTH / 1.5 * Matrix2d::Identity()
		4.2.3 将IMU与相机的时间延迟TD赋值给td

	4.3 imu_callback：
		4.3.1 如果当前的IMU的时间不超过上一个IMU的时间last_imu_t（初值为0），那么直接结束，数据有问题
		4.3.2 将当前的IMU时间赋值给last_imu_t；将imu信息添加到imu_buf中
		4.3.3 predict(imu_msg): 计算预计分量
			1. 
		4.3.4 如果初始化完成后就开始发送IMU的信息了pubLatestOdometry：
			1. 

	4.4 feature_callback：将前端信息送进buffer，第一帧忽略

	4.5 restart_callback：将vins估计器复位
		4.5.1 将feature_buf和imu_buf清空，令current_time为-1，last_imu_t为0

	4.6 relocalization_callback：
		4.6.1 添加点云信息到relo_buf中

	4.7 process：
		4.7.1 对齐IMU和图像数据getMeasurements：
			1. 如果IMU和图像有一个为空，那么直接返回空的测量值measurements；如果imu的最后一帧的时间戳都小于图像的第一帧的时间戳，那么也直接返回空的测量值measurements；如果图像的第一帧时间戳早于IMU的第一帧时间戳，那么将不停地删除图像，直道图像第一帧的时间戳大于IMU第一帧的时间戳
			2. 获取此时图像时间戳前面的所有的IMU的信息到IMUs中，并将这些IMU删除，然后将图像后面（也可能时间戳相等）的一帧IMU信息添加到IMUs中，并将IMUs添加到measurements中
			3. 重复循环，直到满足退出条件
		
		4.7.2 对获取的measurements进行遍历，也就是分别处理两帧之间的IMU信息和当前的图片：
			1. 对当前图像前面的IMU信息，直接送到estimator.processIMU中；对最后一个IMU数据进行插值，正好插值出时间戳为图像时间的IMU的加速度和陀螺仪，并送到estimator.processIMU中：
				1.1 如果是第一帧，那么令first_imu为true，并令acc_0和gyr_0为传入的值；
				1.2 如果当前帧还没有IMU预积分，那么就new一个IntegrationBase
				1.3 如果当前帧是第一帧，那么不处理；否则，将IMU的测量值分别存储到预积分类和estimator中，并做IMU预积分递推P、q和v
				1.4 将IMU测试值更新给acc_0和gyr_0

			2. 如果relo_buf非空，就取出最新的回环帧并赋值给relo_msg，否则relo_msg为NULL；如果relo_msg非空，那么获取回环帧的归一化相机系点集并添加到match_points中，获取回环帧的旋转和平移和frame_index，并执行estimator.setReloFrame：
				2.1 

			3. 对当前图像进行处理：获取当前图像的每个特征点的feature_id、camera_id、去畸变后的归一化相机系坐标、特征点的像素坐标和去畸变后的归一化相机系的特征点的运动速度，并将其添加到image中，然后执行estimator.processImage
				3.1 addFeatureCheckParallax
					3.1.1 对当前图像帧，遍历特征点，如果这个特征点已经在特征点列表feature中，那么直接将特征点信息添加到这个特征点中，并令跟踪到的特征点数目last_trck_num递增；否则，新建一个特征点，并将特征点信息添加到其中，并将这个特征点添加到特征点列表feature中
					3.1.2 如果帧号小于2或者跟踪到的特征点的数量少于20，那么直接结束，认为当前帧是一个
					3.1.3 遍历feature，如果某个特征点能够同时被倒数第三帧和倒数第二帧看到，就执行FeatureManager::compensatedParallax2：
						1. 计算给定的特征点在frame-2和frame-1中的两帧的去畸变的归一化相机系的坐标的距离，用于视差的判断
						2. parallax_num递增
					3.1.4 如果parallax_num为0，那么直接返回true，否则，就计算视差的均值，并与给定的阈值比较，如果大于等于阈值，就返回true；

				3.2 FeaturePerFrame中保存的是特征点的信息（去畸变的归一化相机系坐标和图像坐标和速度）和时间延迟；FeaturePerId中保存的是特征点的id和帧号（new的时候的帧号表示起始帧），还会标记使用的次数，估计的深度（初始值为-1），求解的状态（初始未求解-0，1-求解成功，2-求解失败）

				3.3 如果addFeatureCheckParallax为true，那么就将marginalization_flag置为MARGIN_OLD，否则将其置为MARGIN_SECOND_NEW；并将当前帧的header保存到estimator中的Headers中

				3.4 将当前帧的信息包装成ImageFrame（所有特征点的信息以及时间），令当前帧的预积分为tmp_pre_integration（对于第一帧而言是nullputr，也符合前面的第一帧图像帧之前的IMU信息不处理），后面才开始new，实际上new的是下次进入函数给当前帧的预积分赋值的

				3.5 如果ESTIMATE_EXTRINSIC等于2，并且帧号不等于0
					3.5.1 取出当前帧及其前一帧共同看到的角点FeatureManager::getCorresponding: 
						1. 遍历feature，得到能够同时被frame_count_l和frame_count_r看到的特征点，获取所有的满足条件的特征点在这两帧中的去畸变的归一化相机系坐标

					3.5.2 执行相机与IMU的旋转外参标定CalibrationExRotation：
						1. 根据当前帧与其前一帧的匹配的角点，计算两帧图像之间的相对位姿solveRelativeR：
							1.1 如果匹配数量少于9，那么直接返回单位阵；否则，根据匹配信息，计算本质矩阵E，并对E进行R、t分解，获得四组解
							1.2 testTriangulation：获取内点的比例
								1.2.1 cv::triangulatePoints：执行DLT三角化，计算三维点，并获得深度值为正数的点的比例
							1.3 获得内点比例最多的那一组解并返回此值
						2. 将两帧图像的相对旋转添加到Rc之中，将两帧IMU之间预积分的相对旋转添加到Rimu中；获取上一次优化的ric，并据此计算ric.inverse()*delta_q_imu * ric，这个值理论上应该与此时两帧图像的相对旋转相等，但是由于误差，不会完全相等，将这个值添加到Rc_g中，在我们信任ric（根据前面frame_count - 1帧计算的结果，在frame_count进入之前）的前提，如果仅仅使用frame_count这一帧，也能够计算外参，但是这个值就评价了此时计算的准确性；而后面我们实际上是使用所有的frame_count帧来进行计算，每一帧的图像与imu数据的准确度是不同的，应该使用不同的权重来进行计算，因此，使用这个值来映射权重，构建加权最小二乘
						3. 使用所有的frame_count帧构建加权最小二乘计算外参计算外参，并将结果赋值给ric
						4. 获取A的奇异值中的第三小的那个奇异值，如果其frame_count>=window_size并且这个奇异值大于0.25，那么就将ric赋值给calib_ric_result，并返回true；否则，返回false

					3.5.3 如果标定成功，那么将标定结果存储到ric和RIC中，并将ESTIMATE_EXTRINSIC设置为1

				3.6 如果solver_flag为initial，如果frame_count等于window_size，那么往下面执行，否则frame_count递增
					3.6.1 初始化result为false

					3.6.2 如果ESTIMATE_EXTRINSIC不等于2并且距离initial_timestamp大于0.1秒：
						1. result = initialStructure：
							1.1 判断IMU的激励是否足够：
								1.1.1 对所有的预积分，根据其delta_v计算加速度，并计算这些加速度的方差和标准差
								1.1.2 如果标准差小于0.25，就认为激励不够，但此时不会执行任何逻辑操作，只会打印一条语句

							1.2 执行一个global sfm
								1.2.1 组织特征点的信息：遍历特征点管理器中的所有特征点，对每一个特征点，获取观测到这个特征点的图像的信息（也就是帧号、像素坐标和去畸变的归一化相机系坐标等），并将这些信息存储到sfm_f
								1.2.2 计算枢纽帧，并计算枢纽帧到frame_count对应的那一帧之间的相对位姿relativePose：
									1. 枢纽帧的标准：距离最后一帧不会太远，有足够的共视特征点，用于计算相对位姿；距离又不能太近（用视差判断），太近使用本质矩阵恢复的R、t的稳定性就不够
									
									2. 遍历窗口中的图像，获取这一帧图像与最后一帧之间的共视，如果共视的特征点不大于20，那么就执行下一帧；否则，就计算两帧之间的平均视差，如果平均视差大于30，并且相对位子能够正常计算的话，就认为这一帧就是枢纽帧了，两帧之间相对位姿计算方法solveRelativeRT：
										2.1 如果共视的角点数量小于15，直接返回false，否则
										2.2 利用共视的特征点计算本质矩阵E（使用opencv的接口，并配置ransac模式），然后使用cv::recoverPose恢复R、t：
											2.2.1 使用decomposeEssentialMat计算得到4组解，具体求解方法可以参照SLAM14讲
											2.2.2 将传入的图像坐标根据内参矩阵转换到归一化相机系下
											2.2.3 对每一组解，通过DLT三角化得到三维点，并计算在两个相机下的深度均为正且深度均在50以内的特征点的数量
											2.2.4 与给定的mask做“与”运算（用于剔除不计数的特征点）；返回符合要求的特征点数目最多的那一组解
										2.3 需要注意的是，上面计算的位姿枢纽帧→最后一帧；输出的时候会按照最后一帧→枢纽帧的形式输出
										2.4 计算内点数量，如果内点数量在12以上，那么就认为计算正确，返回true，否则返回false

									3. 如果能够得到满足要求的枢纽帧，那么就返回true，否则返回false

							1.3 进行SFM求解，如果求解失败就将marginalization_flag赋值为MARGIN_OLD并直接结束，否则继续玩后面执行
								1.3.1 设置枢纽帧的相机系为世界系，得到枢纽帧和最后一帧的位姿，并添加到相应变量中；需要注意的是，在后面的处理中，统一使用Tcw，因此需要将位姿都转化为这种形式
								
								1.3.2 从帧号为l（也就是枢纽帧）遍历到倒数第二帧，除了枢纽帧之外，对每一帧做pnp求解位姿，如果求解失败就直接返回false结束，否则，与最后一帧做两帧三角化，注意，枢纽帧不需要再做pnp了，只需要做两帧三角化即可
									1. solveFrameByPnP：
										1.1 使用上一帧的R、t作为初始值；已有的地图点是前面的帧与最后一帧三角化得到，因此当前求解的这一帧一定能够看到这些三维点
										1.2 遍历sfm_f中的特征点，获取其中有深度值的特征点，并将这个特征点在当前帧的归一化相机系坐标和世界系坐标分别保存
										1.3 如果匹配的2D-3D数量不足15，那么会打印“跟踪不稳定”，如果少于10，那么会直接返回false，认为pnp求解失败
										1.4 使用opencv的接口solvePnP求解这个pnp问题，并使用初始值，调用的是CV_ITERATIVE方法，也就是将其建模为重投影误差最小化的问题，使用LM进行求解
										1.5 如果求解失败则返回false，如果求解成功，就将求得的旋转和平移传递给相关参数，并返回true

									2. triangulateTwoFrames：
										2.1 遍历sfm_f中存储的特征点，如果这个点已经三角化了(state为true)，那么直接continue；否则
										2.2 如果这个特征点能够同时被指定的两帧看到，就对这个特征点执行三角化操作，并将三维坐标系保存，然后将其state赋值为true

								1.3.3 从l+1开始直到倒数第二帧，对每一帧，都与枢纽帧l进行三角化，计算相应地图点的深度；前面三角化的特征点都是最后一帧能够看到的，还有大量的最后一帧看不到的特征点没有三角化，因此才会有这里的操作；实际上，这里的操作依然不够完整，因为还有特征点既不能被枢纽帧看到，也不能被最后一帧看到，这些特征点就没有被三角化出来，不过后面会将这些特征点三角化出来

								1.3.4 按照倒序的方式求解枢纽帧前面的图像的位姿，并与枢纽帧做两帧之间的三角化

								1.3.5 遍历sfm_f中的所有特征点，如果其已经三角化了，那么便不处理，如果其观测少于两帧，那么也不处理；否则获取其观测的第一帧和最后一帧，通过两帧三角化将这个特征点的世界系坐标计算出来

								1.3.6 构建一个全局BA用于求解当前这个单目视觉slam系统：
									1. 构建一个problem；添加参数块，包括旋转（使用四元数）和平移，构建重投影误差用于优化（实际上这里是用的是归一化相机系下的残差）

							1.4 计算all_image_frame中所有帧对应的位姿
								1.4.1 如果图像和IMU正好对应，那么直接将这一帧认为是关键帧，并计算IMU的旋转和平移
								1.4.2 如果这一帧没有与IMU对应，那么认为其是普通帧，并令距离其最近的一帧关键帧作为其位姿的初始值，然后遍历其特征点获取其特征点对应的三维点（如果有的话），构建2D-3D匹配，然后调用opencv的solvePnP接口计算位姿
								1.4.3 将计算的结果保存到当前帧的信息中

							1.5 视觉惯性对齐visualInitialAlign，如果对齐正确，就返回true，否则，就返回false：
								1.5.1 视觉和IMU对齐VisualIMUAlignment：
									1 陀螺仪零偏初始化solveGyroscopeBias：
										1.1 思路：
								            1.1.1 通过相机位姿和相机与IMU的外参计算两帧IMU之间的相对旋转（这个不含有陀螺仪零偏）；预积分也能得到两帧之间的相对旋转；二者理论上应该相等
								            1.1.2 由于陀螺仪零偏初始为0，因此不会严格相等，因此可以构建最小二乘进行陀螺仪零偏的求解
								            1.1.3 二者不等的原因当然不仅仅是陀螺仪零偏，还有视觉估计的误差，预积分的误差（中值法本身自带误差）等；因此这里实际上是调整陀螺仪零偏使其适应视觉的结果
								      
								        1.2 计算过程：
								            1.2.1 qbk_c0 = qbk_ck * qck_c0，这里的旋转都是已知的，因此可以计算qbk_c0
								            1.2.2 qbk_bk+1 = qbk_c0 * qc0_bk+1，这里的旋转都是已知的
								            1.2.3 qbk_bk+1.inv * rbk_bk+1的虚部应该是零向量，据此即可构建最小二乘问题，当然rbk_bk+1 = rbk_bk+1_观测 * (1, 0.5 * Jbw * delta_bw)
								            1.2.4 执行变换qbk_bk+1.inv * rbk_bk+1_观测 * (1, 0.5 * Jbw * delta_bw) = I(q) → (1, 0.5 * Jbw * delta_bw) = rbk_bk+1_观测.inv * qbk_bk+1
								            1.2.5 均取虚部即可构建一个Ax = b的模型
								            1.2.6 由于k的取值不唯一，因此模型转变为∑||Ai * x - bi||^2 → x.t() * ∑Ai.t() * Ai * x - ∑(2 * bi.t() * Ai) * x + C
								            1.2.7 令A =  ∑Ai.t() * Ai，B = ∑(bi.t() * Ai)，则问题转化为min(x.t() * A * x - 2 * B * x)
								            1.2.8 上述问题为一个凸优化问题，有闭式解，即求解c

								        1.3 将求解的零偏更新到每一帧的预积分中，并重新预积分一次（类似这种重新预积分的操作，最好少做，非常耗费时间）

									2 视觉惯性对齐LinearAlignment，如果对齐成功就返回true，否则就返回false
										2.1 优化变量是bk坐标系下的第k帧imu的速度（k=0, 1, ..., n），枢纽帧下的重力和尺度
										2.2 通过平移和速度的预积分来构建最小二乘问题，观测数据来自IMU的预积分，平移通过相机与IMU的外参计算得到
										2.3 如果尺度为负数或者求得的重力的模长与标准重力的模长相差超过1，那么认为求解失败，返回false，否则
										2.4 重力精确化操作RefineGravity：
											2.4.1 通过前面获得的重力的方向作为初值，在前面的建模中仅仅改变g_c0 = g_c0 * |g| / |g_c0| + w1 * b1 + w2 * b2，其中|g|表示实际的重力大小，b1和b2和当前的重力方向g_c0垂直
											2.4.2 通过迭代的方式求解速度、重力和尺度
										2.5 获取尺度参数，如果尺度为负数，那么直接返回false，否则返回true

								1.5.2 如果视觉IMU对齐失败，那么直接返回false，否则继续执行

								1.5.3 

						2. 将当前帧的时间戳赋值给initial_timestamp，如果当前初始化失败，那么距离当前帧就得至少0.1秒才能再次初始化

					3.6.3 如果result为true，那么
						1. 将solver_flay设置为NON_LINEAR
						2. solveOdometry：
							2.1 

						3. slideWindow
							3.1 如果marginalization_flag为margin_old：
								3.1.1 获取Headers中的第一帧的时间戳t_0，获取第一帧的旋转back_R0和平移back_P0(相机位姿)
								3.1.2 如果frame_count不等于window_size，那么直接结束；否则
									1. 从0开始遍历到window_size-1，将i对应的信息和i+1对应的信息互换
									2. 将window_size-1对应的信息赋值给window_size对应的变量（作为初值），释放window_size对应的预积分并新建一个预积分，清空dt_buf、linear_acceleration_buf和angular_velocity_buf
									3. 如果solve_flag等于initial，那么：
										3.1 在all_image_frame中寻找t_0对应的那一帧，将其对应的预积分释放；遍历all_image_frame，将it_0前面的帧的预积分全部释放掉；在all_image_frame中将第一帧删除
									4. slideWindowOld:
										4.1 sum_of_back递增；如果solver_flag等于non_linear，那么shift_depth为true，否则为false
										4.2 shift_depth为true：
											4.2.1 根据前面记录的back_R0和back_P0计算被移除帧的旋转R0和平移P0（imu的位姿）；计算当前窗口中的最老帧对应的imu的旋转R1和平移P1
											4.2.2 removeBackShiftDepth：
												1. 遍历所有的特征点，如果特征点不能被移除的那一帧看到，那么只需要将其起始帧递减即可；
												2. 如果特征点能够被移除的那一帧看到，那么首先将其观测中的已经移除的那一帧的信息删除，如果删除后这个特征点的观测的数量少于2，那么直接删除这个特征点，否则，将这个特征点在移除帧下的坐标转换到当前窗口第一帧的坐标系下，如果深度为正，就对estimated_depth正常赋值，否则就给estimated_depth赋值INIT_DEPTH，表示其初始的深度值
										4.3 shift_depth为false，执行removeBack
											4.3.1 遍历特征点，如果这个特征点的起始帧不等于0，也就是不能被已经移除的那一帧看到，那么其起始帧递减即可
											4.3.2 如果这个特征点的起始帧等于0，也就是能够被已经移除的那一帧看到，那么首先在这个特征点的观测中删除有关被移除的那一帧的信息；如果删除后，这个特征点的观测信息为0，那么直接将这个特征点删除
										4.4 注意，在removeBackShiftDepth中，特征点的观测少于2个就删除这个特征点；而在removeBack中，只有没有观测的时候才会删除特征点；可以看出，在还没有初始化的时候，删除的策略趋向于保守

							3.2 如果marginalization_flag为MARGIN_SECOND_NEW：
								3.2.1 如果frame_count不等于window_size，那么直接结束；否则
								3.2.2 合并frame-1和frame对应的imu预积分，并将零偏等对应的信息更新
								3.2.3 释放window_size对应的预积分并new一个新的；释放dt_buf、linear_acceleration_buf和angular_velocity_buf，后续会进行填充
								3.2.4 slideWindowNew：sum_of_front递增，并执行removeFront：
									1. 由于frame_count-1和frame_count合并作为frame_count-1帧了，因此需要对特征点做相应的变动
									2. 遍历所有的特征点，如果这个特征点的起始观测是frame_count，那么直接将起始帧观测递减即可，否则
									3. 判断特征点能够被移除的倒数第二帧（也就是frame_count-1对应的那一帧，并非像slideWindowOld一样，真的被删除了，而是与frame_count对应的那一帧合并了）看到，如果不能就什么也不做，否则，首先移除倒数第二帧的观测，然后判断这个特征点还有没有观测，没有的话就将这个特征点也删除

						4. 移除无效的地图点f_manager.removeFailures：
							4.1 

						5. 存储滑窗里面最新的位姿到last_R和last_P中，存储滑窗中最老的位姿到last_R0和last_P0中

					3.6.4 如果result为false，那么执行slideWindow

				3.7 如果solver_flag不是initial
					3.7.1 求解里程计solveOdometry

					3.7.2 检测VIO是否正常failureDetection；如果检测到不正常：
						1. failure_occur设置为1
						2. clearState：
							2.1 
						3. setParameter：
							3.1 
						4. 直接结束

					3.7.3 求解滑窗slideWindow

					3.7.4 移除无效的地图点f_manager.removeFailures

					3.7.5 清空key_poses，然后将滑窗中的位置存储到key_poses中；存储滑窗里面最新的位姿到last_R和last_P中，存储滑窗中最老的位姿到last_R0和last_P0中


			4. 打印统计信息printStatistics：
				4.1 

			5. 将图像的header的frame_id设置为world，之后执行一些可视化的操作：
				5.1 pubOdometry(estimator, header)
				5.2 pubKeyPoses(estimator, header)
				5.3 pubCameraPoe(estimator, header)
				5.4 pubPointCloud(estimator, header)
				5.5 pubTF(estimator, header)
				5.6 pubKeyframe(estimator)
				5.7 如果relo_msg非空，那么执行pubRelocalization(estimator)

		4.7.3 如果estimator.solver_flag等于NON_LINEAR，那么执行update:
			1. 

			





estimator中的g是在哪里赋值的？







	旋转的几种数学表示方法：
		3.6.1 旋转矩阵：运算方便;使用9个量描述3自由度旋转，引入额外约束，求导困难
		3.6.2 旋转向量：紧凑的表示旋转;具有周期性
		3.6.3 欧拉角：直观，用户友好;需要指定旋转顺序，存在万象节死锁问题，无法进行球面平滑插值
		3.6.4 四元数：紧凑的不带奇异的表示方法;对用户来讲不太直观，要求单位四元数，表示旋转也会过参数化

		
			 

代码中大量存在在归一化坐标系下处理问题：
	1. 计算视差的时候
	2. 初始化的GBA中
这样做并没有理论依据，尤其是如果fx!=fy并且二者相差较大的时候，那么得到的结果将没有理论支持，有可能将一个没有权重的优化问题变为以及加权的优化问题，这是没有道理的




git: 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







1. 在初始化阶段比较难估计的变量有：相机与IMU的平移外参、加速度计的零偏；对于前者可以用尺子量一下，对于后者，即使在初始化阶段忽视它，也不会有太大的问题，后面会一直优化它



目前尚未明白的点：
	1. 零偏只是在一个区间内保持不变进行建模，但是并不是一直不变；因此，程序里面应该有多个零偏需要估计，是否每次开始的时候的零偏都是从0开始估计
	2. 在递推计算tmp_Q的过程中，由于使用了近似计算，使得tmp_Q并不是单位四元数，这是否会对后面的计算产生影响
	3. 此处使用的是归一化的相机坐标系的坐标，如果按照针孔模型的话，要达到10的间隔，说明像素间隔有几千了，是否在前面做了什么处理
	4. 初始化的all image frame是什么，其与初始化的时候的关键帧集合有什么不同
	5. 为什么视觉惯性对齐后又要进行三角化，这是在做什么
	6. 视觉重投影中的T_wbi是什么，这是直接计算的，还是通过了相机；如果是直接计算的，那么两帧之间的距离就不能很远
	7. 为什么旋转有时候是矩阵有时候是四元数，而且求导的方式还不一样，明明参数设置的是四元数，这样做有什么理论依据吗？
	8. 在ORB-SLAM的局部建图BA中，对于局部建图中的地图点，如果有观测不在局部建图中，就将其固定；在VINS-MONO的滑动窗口优化中，为什么要固定窗口中的第一帧，是因为滑动窗口优化中没有使用窗口之外的信息，所以需要固定一帧？
	9. 为什么重力的存在使得自由度为4，不是在静止的时候才有pitch和roll可观吗？很多地方都只优化了yaw和平移，没有优化roll和pitch
	10. 为什么要保持第一帧的yaw不变，为什么平移和速度需要补偿yaw？
	11. 边缘化倒数第二帧是什么意思？


需要后面注意看的点：
	1. 视觉重投影的误差的权重是如何给的：1.5倍


知识点：
	1. VINS里面边缘化后无法使用schur complement技术，是因为帧与帧之间有IMU构建的预积分约束，导致了稠密矩阵的产生；如果仅仅是重投影误差，即使边缘化某些状态，也不会影响到地图点对应的部分的稀疏性



1. 查看VINS-MONO中动态优化Tbc的方法：
	1.1 如果传入参数为2
		1.1.1 没有任何外参信息，
		1.1.2 根据q_cb * q_bkbk+1 = q_ckck+1 * q_cb = q_ckbk+1进行初始化计算旋转外参（平移外参初始化不计算，赋值为0向量），并且在帧数小于WINDOW_SIZE时，重复计算（使用kernel function）
		1.1.3 在后面的滑窗优化中，根据重投影误差进一步优化旋转外参和平移外参
	1.2 如果传入参数为1
		1.2.1 说明有初始值，不进行旋转外参初始化
		1.2.2 在后面的滑窗优化中，根据重投影误差进一步优化旋转外参和平移外参
	1.3 如果传入参数为0
		1.3.1 表示外参非常准确，既不需要进行旋转外参初始化也不需要在后面的滑窗优化中去优化它不需要优化





		ESKF：IMU预积分的结果为predict，重投影误差是update
		在优化里面，IMU的结果产生约束，影响着位姿的优化方向
		
		为什么需要卡尔曼滤波：帮助IMU预积分计算协方差的时候避免四元数过参数化（四元数对应的协方差为4*4）
			1. 使用的轴角是一个三自由度，而四元数是一个过参数的表示（4自由度，需要一个单位四元数的约束）
			2. 误差一般都在0附近，不会有轴角表示中的周期性的问题（theta和theta+2pi等价）
			3. 误差很小，可以很方便的计算易一阶导，省略二阶导也不会导致较大的误差
			4. 误差变化很小，可以不用很高频率的更新





FEJ:
	1. https://www.zhihu.com/question/500852656/answer/2239709690
	2. https://zhuanlan.zhihu.com/p/545544042



