1. 编译：
	1.1 opencv3.4.5: set(CMAKE_PREFIX_PATH) → 修改camera_model的cmakelists.txt的include opencv
		set(CMAKE_PREFIX_PATH "/usr/local/include/opencv3.4.5")
		find_package(OpenCV 3.4)
		if(NOT OpenCV_FOUND)
		    find_package(OpenCV 3.0)
		    if(NOT OpenCV_FOUND)
		        message(FATAL_ERROR "OpenCV > 3.0 not found.")
		    endif()
		endif()
		MESSAGE("OPENCV VERSION:")
		MESSAGE(${OpenCV_VERSION})
	1.2 ceres: https://github.com/ceres-solver/ceres-solver
		1.2.1 mkdir build
		1.2.2 cd build
		1.2.3 cmake ..
		1.2.4 make -j8
		1.2.5 make test
		1.2.6 sudo make install
	1.3 编译vins-mono
		1.3.1 mkdir -p catkin_ws/src
		1.3.2 将vins-mono文件夹放到src下
		1.3.3 catkin_init_workspace
		1.3.4 cd ..
		1.3.5 catkin_make
	1.4 运行测试数据
		1.4.1 新开终端：source devel/setup.bash roscore
		1.4.2 新开终端：source devel/setup.bash roslaunch vins_estimator euroc.launch 
		1.4.3 新开终端：source devel/setup.bash roslaunch vins_estimator vins_rviz.launch
		1.4.4 新开终端：source devel/setup.bash rosbag play YOUR_PATH_TO_DATASET/MH_01_easy.bag 
		1.4.5 为了避免每次都source devel/setup.bash：https://blog.csdn.net/github_53583003/article/details/111187288
			1. sudo  gedit ~/.bashrc
			2. 添加：source ~/devel/setup.bash
	1.5 在clion中打开
		1.5.1 在catkin_ws下，source devel/setup.bash
		1.5.2 ./clion.sh
		1.5.3 使用src下面的camkelists.txt加载
		1.5.4 使用当前窗口，不要新建窗口，新建窗口的话，source就是去意义了，然后正常编译即可

2. camera model
	2.1 抽象类：Camera
		2.1.1 负责创建一些公有变量和公有函数，有一些函数是纯虚函数，继承类必须重写
		2.1.2 创建有大量的用于标定的函数，暂时不学习与标定相关的函数
		2.1.3 比较有意思的是函数reprojectionDist：其评价的是两个相机系下的三维点，在同一个相机下投影到图像上，它们的像素坐标的差值作为重投影误差

	2.2 继承类：PINHOLE
		2.2.1 继承抽象的父类Camera，创建针孔相机
		2.2.2 创建有大量的用于标定的函数，暂时不学习与标定相关的函数；其中函数estimateIntrinsics用于实现张正友博士的Z. Zhang, A Flexible New Technique for Camera Calibration, PAMI 2000
		2.2.3 liftProjective快速去畸变：
			1. 巧妙设计了一种快速去畸变的方法，用于快速求解畸变前的归一化相机系坐标
			2. 注意，当畸变为外扩时，如果第一次修正后的归一化坐标不在当前的1/4图像时，此算法无法收敛；但是需要注意的是，一般畸变都不大（至少不会大到出现跨越图像1/4区域的情况），其次，越靠近图像中心，畸变就越小，因此，这种异常情况是不会发生的

	2.3 继承类：Equidistant，使用的是KB8模型
		2.3.1 继承抽象的父类Camera，创建鱼眼相机，注意其参数是k2~k5与一般的k1~k4不同
		2.3.2 创建有大量的用于标定的函数，暂时不学习与标定相关的函数；其中函数estimateIntrinsics用于实现C. Hughes, P. Denny, M. Glavin, and E. Jones, Equidistant Fish-Eye Calibration and Rectification by Vanishing Point；Extraction, PAMI 2010 Find circles from rows of chessboard corners, and for each pair of circles, find vanishing points: v1 and v2. 
		2.3.3 liftProjective
			1. 将像素坐标（u, v）变换为畸变后的归一化相机系坐标(xd, yd)
			2. 使用backprojectSymmetric求解theta和phi
			3. 将(xd, yd)通过theta和phi变化到去畸变前的相机归一化坐标系下，并单位化，从而变换到球面上
		2.3.4 backprojectSymmetric
			1. 相机系下的三维点A与光心的连线与归一化相机系平面交于B1（对应r），经过畸变后与归一化相机系平面交于B2（对应theta_d）；有B1、B2和归一化平面的原点O三点共线
			2. 由于O、B1和B2共线，所以可以直接通过xd, yd计算phi
			3. 对theta的求解：将关于theta的多项式函数令为矩阵A的特征多项式（转化方法见代码），然后调用Eigen的接口求解最小的特征多项式

	2.4 继承类：Scaramuzza --- 暂时不深究
		2.4.1 继承抽象的父类Camera，创建全景相机模型Scaramuzza
		2.4.2 参考教程为：
			1. https://blog.csdn.net/j879159541/article/details/125566948
			2. https://sites.google.com/site/scarabotix/ocamcalib-toolbox

	2.5 继承类：CataCamera --- 暂时不深究
		2.5.1 继承抽象的父类Camera，创建全景相机模型Mei
		2.5.2 参考教程为：
			1. https://blog.csdn.net/j879159541/article/details/125409410
			2. C. Mei, and P. Rives, Single View Point Omnidirectional Camera Calibration from Planar Grids, ICRA 2007

3. feature tracker
	3.1 特征点存储信息：
		3.1.1 像素坐标 → 特征点提取算法
		3.1.2 去畸变后归一化坐标 → 特征点去畸变算法
		3.1.3 特征点id → 光流跟踪算法
		3.1.4 特征点速度 → 时间同步算法使用

	3.2 快门（shutter）: https://blog.csdn.net/weixin_43503508/article/details/108014615
		3.2.1 全局快门（Global Shutter）
			1. 通过整幅场景在同一时间曝光实现的。Sensor所有像素点同时收集光线，同时曝光。即在曝光开始的时候，Sensor开始收集光线；在曝光结束的时候，光线收集电路被切断。然后Sensor值读出即为一幅照片。CCD就是Global shutter工作方式。所有像元同时曝光。
			2. 曝光时间更短，但会增加读出噪声；
		3.2.2 卷帘快门（Rolling Shutter）
			1. 与Global shutter不同，它是通过Sensor逐行曝光的方式实现的。在曝光开始的时候，Sensor逐行扫描逐行进行曝光，直至所有像素点都被曝光。当然，所有的动作在极短的时间内完成。不同行像元的曝光时间不同。
			2. 对于相机厂家，Rolling shutter可以达到更高的帧速，但当曝光不当或物体移动较快时，会出现部分曝光(partial exposure)、斜坡图形(skew)、晃动(wobble) 等现象。这种Rolling shutter方式拍摄出现的现象，就定义为果冻效应。
		3.2.3 曝光时间短的应用（如<500μs）适合Global shutter，曝光时间长（如大于500μs）时，选择rolling shutter可以有更低的噪声和帧速。
		3.2.4 相机曝光时间：
			1. 从快门打开到关闭的时间间隔，在这一段时间内，物体可以在底片上留下影像，曝光时间是看需要而定的，没有长短好坏的说法只有需要的讲法。比如你拍星星的轨迹，就需要很长的曝光时间（可能是几个小时），这样星星的长时间运动轨迹就会在底片上成像。如果你要拍飞驰的汽车清晰的身影就要用很短的时间（通常是几千分之一秒）。曝光时间长的话进的光就多，适合光线条件比较差的情况。曝光时间短则适合光线比较好的情况。
			2. 参考教程：https://baike.baidu.com/item/%E6%9B%9D%E5%85%89%E6%97%B6%E9%97%B4/485425?fr=aladdin
		3.2.5 快门是照相机用来控制感光片有效曝光时间的机构。是照相机的一个重要组成部分，它的结构、形式及功能是衡量照相机档次的一个重要因素。一般而言快门的时间范围越大越好。秒数低适合拍运动中的物体，可轻松抓住急速移动的目标。不过当你要拍的是夜晚的车水马龙，快门时间就要拉长，常见照片中丝绢般的水流效果也要用慢速快门才能拍出来。

	3.3 readParameters
		3.3.1 读取配置文件的信息：image_topic、imu_topic等
		3.3.2 将配置文件路径添加到CAM_NAMES中，实际上这是一个单目VIO系统，CAM_NAMES的size仅仅为1
		3.3.3 设置WINDOW_SIZE(20)、FOCAL_LENGTH(460)

	3.4 readIntrinsicParameter：创建相机，并读取相机的内参数、畸变稀疏和相机类型等

	3.5 img_callback
		3.5.1 第一帧的时候，first_image_flag为true，此时，将时间戳存储到first_image_time和last_image_time，并令first_image_flag为false，然后直接结束
		3.5.2 如果当前帧的时间戳减去上一帧的时间戳（last_image_time）大于1秒（光流法需要两帧之间的间距尽可能小）或者当前时间戳小于上一帧时间戳，那么执行reset为初始值：first_iamge_flag为true，last_image_time为0，pub_count为1，将重启告诉其他模块，并直接结束
		3.5.3 更新last_image_time，并维持发给后端的图像数量不会超过指定的频率；将ros message转化为cv::Mat
		3.5.4 createCLAHE：防止图像太亮或者太暗，不利于提取特征点
		    1. 对图像进行分块后执行equalizeHist
		    	1.1 对图片灰度图做频数统计，计算器频率，然后根据像素灰度值进行排列
		        1.2 根据像素灰度值计算累计概率，利用公式：new_gray = acc_p * (255 - 0)
		        1.3 这样做的目的是使得灰度值之间的间隔更小，即将一些频数较大的灰度值补充给了频数较小的灰度值，从而实现了灰度值的均衡化
		        1.4 由于整体亮度的提升，也会使得局部图像的细节变的模糊，因此最好进行分块的局部均衡化，参考createCLAHE

		    2. 参考教程：
		    	3.1 https://www.cnblogs.com/my-love-is-python/p/10405811.html
		    	3.2 https://en.wikipedia.org/wiki/Histogram_equalization

		3.5.5 readImage:读取图像，并做光流跟踪
			1. 获取当前时间，并且如果EQUALIZE为true，那么就对图像执行createCLAHE，并将结果赋值给img

			2. 更新forw_img，如果是第一帧的话，直接将图像赋值给cur_img和forw_img；如果上一帧有特征点的话：
				2.1 执行光流跟踪calcOpticalFlowPyrLK：使用图像金字塔进行光流追踪，上层结果为下层初值，提升光流追踪的稳定性，这里使用的4层
					2.1.1 光流与特征点的对比：
						1. 光流根据前一张图片的特征点在本张图片中搜索其对应的匹配点，由于不是所有的特征点都能匹配到，因此还需要重新提取一些特征点，因此对于每张图片，其过程为：LK光流追踪，重新提取不足的部分特征点
						2. 对于特征点而言，每张图片都需要提取相同数量的特征点，还需要就是那描述子，然后还需要计算匹配（匹配比较耗时）
						3. 总结：
							3.1 特征点法会比光流耗时
							3.2 光流法在特征点的追踪上会更加鲁棒
							3.3 光流应用于两帧运动很小的时候，而在重定位和回环检测的运动很大，此时更适合用特征点法
							3.4 如果存在遮挡的时候，特征点法会更好；例如，1→2→3，帧1、3看到了某个地图点，2没有看到，如果使用光流法，就不会认为帧1、3看到的是用一个地图点，因为2没有看到，会认为是一个新的地图点；而此时在特征点法中会认为是同一个地图点

				2.2 利用光流跟踪返回的状态值和图像边界删除外点

				2.3 利用reduceVector对cur_pts、forw_pts、ids、cur_un_pts和track_cnt的规模进行删减，并且不会改变原有的顺序：
					2.3.1 利用双指针实现，j指针指向数据，i指针指向状态，只有状态为true的时候，才会将v[j]=v[i]，类似于用v[i]覆盖v[j]
					2.3.2 对数据resize即可

			3. 如果publish这一帧的话，那么：
				3.1 rejectWithF：对极约束剔除外点，第一帧没有对极约束
					3.1.1 只有跟踪到的点的数量不少于8个的时候才会执行
					3.1.2 获取当前帧和前一帧跟踪到的点的liftProjective结果（不同的相机，结果的表示是不同的；比如针孔相机的结果是归一化相机系坐标；鱼眼相机是归一化相机系坐标往单位球上投影的结果），并将结果往虚拟相机上进行投影；构建虚拟相机的目的是使得对极几何等的阈值不受fx和fy的影响，在一个统一的FOCAL_LENGTH下使用一个固定的F_THRESHOLD
					3.1.3 使用opencv的接口findFundamentalMat，在Ransac的模式下构建对极几何，并使用F_THRESHOLD作为阈值；根据对极几何的结果status对prev_pts、cur_pts、forw_pts、cur_un_pts、ids和track_cnt进行处理

				3.2 setMask：特征点均匀化，第一帧不做均匀化
					3.2.1 获取mask，如果没有mask的话，就直接创建一张白图（也就是允许所有的点通过）
					3.2.2 构建变量cnt_pts_id，保存的是pair<跟踪的次数, pair<特征点,id>>，然后根据跟踪的次数进行排序，由大到小；然后清空forw_pts、ids和track_cnt
					3.2.3 对cnt_pts_id进行遍历，根据mask选择是否保留，如果保留这个特征点的话，就记录特征点、id和跟踪的次数；然后在这个特征点上画圈，圈内的特征点不会被获取了

				3.3 根据所需的特征点数量减去跟踪到的特征点数量，获得还需要提取的特征点数量，然后执行特征点提取操作goodFeaturesToTrack：
					3.3.1 

				3.4 将提取的角点添加到相应的容器中addPoints：
					3.4.1 对新提取的特征点，将其添加到forw_pts、ids和track_cnt中，注意ids中添加的是-1，在后面updateID中进行赋值；track_cnt添加的是1，因为是新增的特征点，还没有被跟踪到

			4. 更新cur_img为forw_img，cur_pts为forw_pts；对特征点进行去畸变undistortedPoints:
				4.1 清空cur_un_pts和cur_un_pts_map
				4.2 遍历cur_pts，对每一个特征点进行去畸变，并将结果保存在cur_un_pts和cur_un_pts_map中
				4.3 计算特征点的运动速度：使用去畸变后的图像坐标进行计算
					4.3.1 对第一帧而言，所有特征点的速度为0
					4.3.2 从第二帧开始，首先清空pts_velocity，计算时间间隔dt，遍历cur_un_pts，如果其是跟踪到的，那么计算速度，如果不是跟踪到的，那么速度直接为0，将速度添加到pts_velocity中，更新prev_un_pts_map为cur_un_pts_map

			5. 更新prev_time为cur_time；令人比较迷惑的是，cur_img和cur_pts表示上一帧的图像和特征点，而cur_time表示当前时间

		3.5.6 给新的特征点赋上ID，updateID：第一帧统一在这里进行赋值操作
			1. 如果没有越界，并且是新的特征点，那么id=n_id，并且n_id递增，并返回true；否则，直接返回false

		3.5.7 向后端发送数据：只发送跟踪大于1的特征点，发送的信息有去畸变后的归一化相机系坐标、ID、图形的u坐标、图像的v坐标、u轴的特征点速度和v轴的特征点速度

		3.5.8 可视化：绘制图像和特征点，特征点的颜色根据跟踪次数决定，也就是特征点的颜色并不单一

4. vins_estimator
	4.1 readParameters：
		4.1.1 获取单次优化最大求解时间、单次优化最大迭代次数、关键帧视差（用于确定关键帧的阈值：fsSettings["keyframe_parallax"]/FOCAL_LENGTH）、输出文件的路径（如输出外参动态标定的结果）
		4.1.2 读取IMU参数，如陀螺仪和加速度的噪声和随机游走；图片宽高
		4.1.3 是否估计IMU与相机的外参
			1. 如果ESTIMATE_EXTRINSIC为2，将单位阵添加到RIC，将零向量添加到TIC
			2. 如果ESTIMATE_EXTRINSIC为1或者0，读取给定的旋转和平移分别添加到RIC和TIC中
			3. ESTIMATE_EXTRINSIC=2将会从单位阵开始优化；ESTIMATE_EXTRINSIC=1将会从给定值开始优化；ESTIMATE_EXTRINSIC=0不会优化外参
		4.1.4 读取初始深度INIT_DEPTH、BIAS_ACC_THRESHOLD、BIAS_GYR_THRESHOLD
		4.1.5 读取IMU与相机的时间延时（image + TD = IMU），并读取是否优化时间延时
		4.1.6 读取相机是否是卷帘相机，如果是的话读取卷帘相机的参数TR（读取每行的时间）

	4.2 setParameter：
		4.2.1 将读取的外参赋值给tic和ric
		4.2.2 对ProjectionFactor::sqrt_info和ProjectionTdFactor::sqrt_info均赋值为FOCAL_LENGTH / 1.5 * Matrix2d::Identity()
		4.2.3 将IMU与相机的时间延迟TD赋值给td

	4.3 imu_callback：
		4.3.1 如果当前的IMU的时间不超过上一个IMU的时间last_imu_t（初值为0），那么直接结束，数据有问题
		4.3.2 将当前的IMU时间赋值给last_imu_t；将imu信息添加到imu_buf中
		4.3.3 predict(imu_msg): 计算预计分量
			1. 
		4.3.4 如果初始化完成后就开始发送IMU的信息了pubLatestOdometry：
			1. 

	4.4 feature_callback：将前端信息送进buffer，第一帧忽略

	4.5 restart_callback：将vins估计器复位
		4.5.1 将feature_buf和imu_buf清空，令current_time为-1，last_imu_t为0

	4.6 relocalization_callback：
		4.6.1 添加点云信息到relo_buf中

	4.7 process：
		4.7.1 对齐IMU和图像数据getMeasurements：
			1. 如果IMU和图像有一个为空，那么直接返回空的测量值measurements；如果imu的最后一帧的时间戳都小于图像的第一帧的时间戳，那么也直接返回空的测量值measurements；如果图像的第一帧时间戳早于IMU的第一帧时间戳，那么将不停地删除图像，直道图像第一帧的时间戳大于IMU第一帧的时间戳
			2. 获取此时图像时间戳前面的所有的IMU的信息到IMUs中，并将这些IMU删除，然后将图像后面（也可能时间戳相等）的一帧IMU信息添加到IMUs中，并将IMUs添加到measurements中
			3. 重复循环，直到满足退出条件
		
		4.7.2 对获取的measurements进行遍历，也就是分别处理两帧之间的IMU信息和当前的图片：
			1. 对当前图像前面的IMU信息，直接送到estimator.processIMU中；对最后一个IMU数据进行插值，正好插值出时间戳为图像时间的IMU的加速度和陀螺仪，并送到estimator.processIMU中：
				1.1 

			2. 如果relo_buf非空，就取出最新的回环帧并赋值给relo_msg，否则relo_msg为NULL；如果relo_msg非空，那么获取回环帧的归一化相机系点集并添加到match_points中，获取回环帧的旋转和平移和frame_index，并执行estimator.setReloFrame：
				2.1 

			3. 对当前图像进行处理：获取当前图像的每个特征点的feature_id、camera_id、去畸变后的归一化相机系坐标、特征点的像素坐标和去畸变后的归一化相机系的特征点的运动速度，并将其添加到image中，然后执行estimator.processImage
				3.1 

			4. 打印统计信息printStatistics：
				4.1 

			5. 将图像的header的frame_id设置为world，之后执行一些可视化的操作：
				5.1 pubOdometry(estimator, header)
				5.2 pubKeyPoses(estimator, header)
				5.3 pubCameraPose(estimator, header)
				5.4 pubPointCloud(estimator, header)
				5.5 pubTF(estimator, header)
				5.6 pubKeyframe(estimator)
				5.7 如果relo_msg非空，那么执行pubRelocalization(estimator)

		4.7.3 如果estimator.solver_flag等于NON_LINEAR，那么执行update:
			1. 

			





estimator中的g是在哪里赋值的？







	旋转的几种数学表示方法：
		3.6.1 旋转矩阵：运算方便;使用9个量描述3自由度旋转，引入额外约束，求导困难
		3.6.2 旋转向量：紧凑的表示旋转;具有周期性
		3.6.3 欧拉角：直观，用户友好;需要指定旋转顺序，存在万象节死锁问题，无法进行球面平滑插值
		3.6.4 四元数：紧凑的不带奇异的表示方法;对用户来讲不太直观，要求单位四元数，表示旋转也会过参数化

		
			 






git: 







1. 在初始化阶段比较难估计的变量有：相机与IMU的平移外参、加速度计的零偏；对于前者可以用尺子量一下，对于后者，即使在初始化阶段忽视它，也不会有太大的问题，后面会一直优化它



目前尚未明白的点：
	1. 零偏只是在一个区间内保持不变进行建模，但是并不是一直不变；因此，程序里面应该有多个零偏需要估计，是否每次开始的时候的零偏都是从0开始估计
	2. 在递推计算tmp_Q的过程中，由于使用了近似计算，使得tmp_Q并不是单位四元数，这是否会对后面的计算产生影响
	3. 此处使用的是归一化的相机坐标系的坐标，如果按照针孔模型的话，要达到10的间隔，说明像素间隔有几千了，是否在前面做了什么处理
	4. 初始化的all image frame是什么，其与初始化的时候的关键帧集合有什么不同
	5. 为什么视觉惯性对齐后又要进行三角化，这是在做什么
	6. 视觉重投影中的T_wbi是什么，这是直接计算的，还是通过了相机；如果是直接计算的，那么两帧之间的距离就不能很远
	7. 为什么旋转有时候是矩阵有时候是四元数，而且求导的方式还不一样，明明参数设置的是四元数，这样做有什么理论依据吗？
	8. 在ORB-SLAM的局部建图BA中，对于局部建图中的地图点，如果有观测不在局部建图中，就将其固定；在VINS-MONO的滑动窗口优化中，为什么要固定窗口中的第一帧，是因为滑动窗口优化中没有使用窗口之外的信息，所以需要固定一帧？
	9. 为什么重力的存在使得自由度为4，不是在静止的时候才有pitch和roll可观吗？很多地方都只优化了yaw和平移，没有优化roll和pitch
	10. 为什么要保持第一帧的yaw不变，为什么平移和速度需要补偿yaw？
	11. 边缘化倒数第二帧是什么意思？


需要后面注意看的点：
	1. 视觉重投影的误差的权重是如何给的：1.5倍


知识点：
	1. VINS里面边缘化后无法使用schur complement技术，是因为帧与帧之间有IMU构建的预积分约束，导致了稠密矩阵的产生；如果仅仅是重投影误差，即使边缘化某些状态，也不会影响到地图点对应的部分的稀疏性



1. 查看VINS-MONO中动态优化Tbc的方法：
	1.1 如果传入参数为2
		1.1.1 没有任何外参信息，
		1.1.2 根据q_cb * q_bkbk+1 = q_ckck+1 * q_cb = q_ckbk+1进行初始化计算旋转外参（平移外参初始化不计算，赋值为0向量），并且在帧数小于WINDOW_SIZE时，重复计算（使用kernel function）
		1.1.3 在后面的滑窗优化中，根据重投影误差进一步优化旋转外参和平移外参
	1.2 如果传入参数为1
		1.2.1 说明有初始值，不进行旋转外参初始化
		1.2.2 在后面的滑窗优化中，根据重投影误差进一步优化旋转外参和平移外参
	1.3 如果传入参数为0
		1.3.1 表示外参非常准确，既不需要进行旋转外参初始化也不需要在后面的滑窗优化中去优化它不需要优化





		ESKF：IMU预积分的结果为predict，重投影误差是update
		在优化里面，IMU的结果产生约束，影响着位姿的优化方向
		
		为什么需要卡尔曼滤波：帮助IMU预积分计算协方差的时候避免四元数过参数化（四元数对应的协方差为4*4）
			1. 使用的轴角是一个三自由度，而四元数是一个过参数的表示（4自由度，需要一个单位四元数的约束）
			2. 误差一般都在0附近，不会有轴角表示中的周期性的问题（theta和theta+2pi等价）
			3. 误差很小，可以很方便的计算易一阶导，省略二阶导也不会导致较大的误差
			4. 误差变化很小，可以不用很高频率的更新





FEJ:
	1. https://www.zhihu.com/question/500852656/answer/2239709690
	2. https://zhuanlan.zhihu.com/p/545544042



